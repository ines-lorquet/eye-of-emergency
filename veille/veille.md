### Q4 : Quâ€™est-ce quâ€™un stop-word ? Pourquoi est-il important de supprimer les stop-words ?

En recherche d'information, un stop-word (ou mot vide) est un mot tellement courant qu'il apporte peu ou pas d'information sÃ©mantique utile. Il est donc souvent inutile de lâ€™indexer ou de le prendre en compte dans une recherche.  
En franÃ§ais, des mots vides typiques sont par exemple : Â« le Â», Â« la Â», Â« de Â», Â« du Â», Â« ce Â».

***Exemple :***  

**Version non nettoyÃ©e :**  
Â« Je suis allÃ© Ã  la bibliothÃ¨que avec mon frÃ¨re, mais il n'y avait pas de livres intÃ©ressants. Â»

**Stop-words extraits :**  
je, suis, Ã , la, avec, mon, mais, il, n', y, avait, de

**Version nettoyÃ©e (sans stop-words) :**  
Â« allÃ© bibliothÃ¨que frÃ¨re, livres intÃ©ressants. Â»

Ce traitement est typique dans le prÃ©traitement NLP (traitement automatique du langage naturel) : il permet de se concentrer sur les mots clÃ©s qui portent rÃ©ellement le sens du texte.
  
***Impact avec et sans stop word :***  

![Visualiser l'impact avec et sans](img/stop_word_graph.webp "Visualiser l'impact avec et sans")



### Q5 : En plus de la ponctuation, on retrouve souvent des caractÃ¨res spÃ©ciaux au sein de donnÃ©es textuelles. Comment sont traitÃ©s ces deux types de caractÃ¨res ?

En traitement de texte, la ponctuation et les caractÃ¨res spÃ©ciaux peuvent nuire Ã  lâ€™analyse automatique. Ils compliquent la sÃ©paration correcte des mots (tokenisation) et ajoutent du bruit dans les donnÃ©es, ce qui gÃªne la comprÃ©hension par les machines.

Ces Ã©lÃ©ments n'ont gÃ©nÃ©ralement pas de valeur sÃ©mantique importante, câ€™est pourquoi ils sont souvent supprimÃ©s ou normalisÃ©s lors du prÃ©traitement. Cela permet dâ€™obtenir un texte plus propre, facilitant les tÃ¢ches comme lâ€™analyse de sentiment ou la classification.

**CaractÃ¨res spÃ©ciaux et de ponctuation qui sont souvent supprimÃ©s :**  
| # â€˜â€œ , .% & ^*! @ ( ) _ = â€“ [ ]$ > \ { } ` ~; : / ? <


***Exemple :***  

**Version non nettoyÃ©e :**  
"Salut !!! Tu vas bien ? Moi Ã§a va ðŸ˜Š. On se voit @18h30 â€“ n'oublie pas ðŸ˜‰ #rdv"

**Ã‰tapes de traitement :**  

    - Suppression de la ponctuation excessive (!!!, ?, ., â€“, â€™, etc.)

    - Suppression des emojis (ðŸ˜Š, ðŸ˜‰)

    - Suppression des caractÃ¨res spÃ©ciaux / hashtags / mentions (@, #)

    - Nettoyage des contractions ou symboles non standards 

**Version nettoyÃ©e (sans ponctation et caractÃ¨res spÃ©ciaux):**  
"Salut Tu vas bien Moi Ã§a va On se voit 18h30 n'oublie pas rdv"



### Q6 : Quâ€™est ce quâ€™un token ? un N-gram ? Quel processus permet-il de les obtenir ?

Un token est une unitÃ© de base dâ€™un texte aprÃ¨s dÃ©coupage, souvent un mot, mais cela peut aussi Ãªtre un sous-mot ou un caractÃ¨re selon le contexte.
Un N-gram est une sÃ©quence de n tokens consÃ©cutifs. Par exemple, un bigramme (2-gram) contient deux mots consÃ©cutifs, un trigramme en contient trois, etc.

Ces Ã©lÃ©ments sont obtenus par un processus appelÃ© tokenisation. Câ€™est une Ã©tape du prÃ©traitement en NLP qui dÃ©coupe le texte brut en unitÃ©s exploitables par les algorithmes.

***Exemple Token :***  

**Texte brut :**  
"Les chats dorment souvent au soleil."

**AprÃ¨s tokenisation (tokens) :**  
["Les", "chats", "dorment", "souvent", "au", "soleil"]
![Visualiser l'impact avec et sans](img/Tokenisation.png "Visualiser l'impact avec et sans")

***Exemples N-gram :***  

**Bigrams (2-grams) :**  
["Les chats", "chats dorment", "dorment souvent", "souvent au", "au soleil"]

**Trigrams (3-grams) :**  
["Les chats dorment", "chats dorment souvent", "dorment souvent au", "souvent au soleil"]

![Visualiser l'impact avec et sans](img/n_gram.png "Visualiser l'impact avec et sans")

Ce dÃ©coupage permet aux modÃ¨les de mieux comprendre le contexte et les relations entre les mots dans une phrase.



