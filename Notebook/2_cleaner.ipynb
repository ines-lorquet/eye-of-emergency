{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a290af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "import string\n",
    "from wordcloud import STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27635f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_pickle(\"df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39130f5",
   "metadata": {},
   "source": [
    "# Traitement des valeurs nuls ou non pertinentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02cab5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "387d3516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 17)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "078faccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5080, 17)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5080 entries, 31 to 7581\n",
      "Data columns (total 17 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   id                 5080 non-null   int64  \n",
      " 1   keyword            5080 non-null   object \n",
      " 2   location           5080 non-null   object \n",
      " 3   text               5080 non-null   object \n",
      " 4   target             5080 non-null   int64  \n",
      " 5   presence_location  5080 non-null   int64  \n",
      " 6   presence_keyword   5080 non-null   int64  \n",
      " 7   char_count         5080 non-null   int64  \n",
      " 8   word_count         5080 non-null   int64  \n",
      " 9   stopword_count     5080 non-null   int64  \n",
      " 10  unique_word_count  5080 non-null   int64  \n",
      " 11  stop_word_count    5080 non-null   int64  \n",
      " 12  url_count          5080 non-null   int64  \n",
      " 13  mean_word_length   5080 non-null   float64\n",
      " 14  punctuation_count  5080 non-null   int64  \n",
      " 15  hashtag_count      5080 non-null   int64  \n",
      " 16  mention_count      5080 non-null   int64  \n",
      "dtypes: float64(1), int64(13), object(3)\n",
      "memory usage: 714.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "print(df.shape) \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792b6963",
   "metadata": {},
   "source": [
    "## ⚠️ Alternative : Gestion intelligente des valeurs manquantes\n",
    "\n",
    "Au lieu de supprimer 33% des données avec `dropna()`, considérons cette approche :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74632885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative recommandée : imputation intelligente\n",
    "df_alternative = df.copy()\n",
    "\n",
    "# 1. Remplacer les valeurs manquantes par des chaînes vides ou des valeurs par défaut\n",
    "df_alternative['location'] = df_alternative['location'].fillna(\"unknown_location\")\n",
    "df_alternative['keyword'] = df_alternative['keyword'].fillna(\"no_keyword\")\n",
    "\n",
    "print(f\"Données conservées : {df_alternative.shape[0]} lignes (vs {df.shape[0]} avec dropna)\")\n",
    "print(f\"Pourcentage conservé : {df_alternative.shape[0]/7613*100:.1f}%\")\n",
    "\n",
    "# 2. Créer des indicateurs binaires de présence\n",
    "df_alternative['has_location'] = df['location'].notnull().astype(int)\n",
    "df_alternative['has_keyword'] = df['keyword'].notnull().astype(int)\n",
    "\n",
    "print(\"\\nNouvelles features créées :\")\n",
    "print(\"- has_location : indique si l'utilisateur a renseigné sa localisation\")\n",
    "print(\"- has_keyword : indique si un keyword était disponible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a6b2b6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>presence_location</th>\n",
       "      <th>presence_keyword</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>stopword_count</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>stop_word_count</th>\n",
       "      <th>url_count</th>\n",
       "      <th>mean_word_length</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>mention_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>48</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>@bbcmtd Wholesale Markets ablaze http://t.co/l...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>49</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Est. September 2012 - Bristol</td>\n",
       "      <td>We always try to bring the heavy. #metal #RT h...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>67</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>50</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>AFRICA</td>\n",
       "      <td>#AFRICANBAZE: Breaking news:Nigeria flag set a...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8.222222</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>52</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Philadelphia, PA</td>\n",
       "      <td>Crying out for more! Set me ablaze</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>53</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>London, UK</td>\n",
       "      <td>On plus side LOOK AT THE SKY LAST NIGHT IT WAS...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4.923077</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id keyword                       location  \\\n",
       "31  48  ablaze                     Birmingham   \n",
       "32  49  ablaze  Est. September 2012 - Bristol   \n",
       "33  50  ablaze                         AFRICA   \n",
       "34  52  ablaze               Philadelphia, PA   \n",
       "35  53  ablaze                     London, UK   \n",
       "\n",
       "                                                 text  target  \\\n",
       "31  @bbcmtd Wholesale Markets ablaze http://t.co/l...       1   \n",
       "32  We always try to bring the heavy. #metal #RT h...       0   \n",
       "33  #AFRICANBAZE: Breaking news:Nigeria flag set a...       1   \n",
       "34                 Crying out for more! Set me ablaze       0   \n",
       "35  On plus side LOOK AT THE SKY LAST NIGHT IT WAS...       0   \n",
       "\n",
       "    presence_location  presence_keyword  char_count  word_count  \\\n",
       "31                  1                 1          55           5   \n",
       "32                  1                 1          67          10   \n",
       "33                  1                 1          82           9   \n",
       "34                  1                 1          34           7   \n",
       "35                  1                 1          76          13   \n",
       "\n",
       "    stopword_count  unique_word_count  stop_word_count  url_count  \\\n",
       "31               0                  5                0          1   \n",
       "32               4                 10                3          1   \n",
       "33               1                  9                1          1   \n",
       "34               3                  7                3          0   \n",
       "35               7                 13                5          1   \n",
       "\n",
       "    mean_word_length  punctuation_count  hashtag_count  mention_count  \n",
       "31         10.200000                  6              0              1  \n",
       "32          5.800000                  8              2              0  \n",
       "33          8.222222                  9              1              0  \n",
       "34          4.000000                  1              0              0  \n",
       "35          4.923077                  5              0              0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ea0176d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    0.567717\n",
       "1    0.432283\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d9f971",
   "metadata": {},
   "source": [
    "# Proportion target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ff86436",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_0 = df[df['target'] == 0]\n",
    "df_1 = df[df['target'] == 1]\n",
    "\n",
    "# 2. Échantillonner aléatoirement la classe 0 pour avoir autant d'exemples que la classe 1\n",
    "df_0_sampled = df_0.sample(n=len(df_1), random_state=42)\n",
    "\n",
    "df_balanced = pd.concat([df_0_sampled, df_1], axis=0).sample(frac=1, random_state=42)  # shuffle le tout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3735ce77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "1    0.5\n",
       "0    0.5\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_balanced['target'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "99e66011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keyword\n",
       "collision                33\n",
       "outbreak                 31\n",
       "sandstorm                31\n",
       "fatalities               30\n",
       "emergency%20plan         29\n",
       "                         ..\n",
       "razed                     9\n",
       "detonation                8\n",
       "epicentre                 7\n",
       "radiation%20emergency     6\n",
       "inundation                3\n",
       "Name: count, Length: 221, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_balanced['keyword'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b1b7f6",
   "metadata": {},
   "source": [
    "## Nettoyage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90fc1772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sulivanmoreau/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)      # URLs\n",
    "    text = re.sub(r\"@\\w+|#\\w+\", \"\", text)           # Mentions & hashtags\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)            # Ponctuation / chiffres\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# Nettoyage de base\n",
    "df_balanced[\"clean_text\"] = df_balanced[\"text\"].apply(clean_text)\n",
    "df_balanced[\"clean_location\"] = df_balanced[\"location\"].apply(clean_text)\n",
    "df_balanced[\"clean_keyword\"] = df_balanced[\"keyword\"].apply(clean_text)\n",
    "\n",
    "# Tokenisation X\n",
    "df_balanced[\"tokens\"] = df_balanced[\"clean_text\"].apply(lambda x: x.split())\n",
    "\n",
    "# Suppression des stopwords\n",
    "df_balanced[\"tokens\"] = df_balanced[\"tokens\"].apply(lambda tokens: [w for w in tokens if w not in stop_words])  \n",
    "df_balanced[\"keyword_tokens\"] = df_balanced[\"clean_keyword\"].apply(lambda x: [w for w in x.split() if w not in stop_words])\n",
    "df_balanced[\"location_tokens\"] = df_balanced[\"clean_location\"].apply(lambda x: [w for w in x.split() if w not in stop_words])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44daffba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>presence_location</th>\n",
       "      <th>presence_keyword</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>stopword_count</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_word_length</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_location</th>\n",
       "      <th>clean_keyword</th>\n",
       "      <th>tokens</th>\n",
       "      <th>keyword_tokens</th>\n",
       "      <th>location_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6818</th>\n",
       "      <td>9765</td>\n",
       "      <td>trapped</td>\n",
       "      <td>10 Steps Ahead.  Cloud 9</td>\n",
       "      <td>Bomb head? Explosive decisions dat produced mo...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>134</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>5.428571</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>bomb head explosive decisions dat produced mor...</td>\n",
       "      <td>steps ahead cloud</td>\n",
       "      <td>trapped</td>\n",
       "      <td>[bomb, head, explosive, decisions, dat, produc...</td>\n",
       "      <td>[trapped]</td>\n",
       "      <td>[steps, ahead, cloud]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4842</th>\n",
       "      <td>6896</td>\n",
       "      <td>mass%20murder</td>\n",
       "      <td>Huntsville, AL</td>\n",
       "      <td>Okay not sure the word 'mass murder' applies d...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>121</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>5.421053</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>okay not sure the word mass murder applies dur...</td>\n",
       "      <td>huntsville al</td>\n",
       "      <td>massmurder</td>\n",
       "      <td>[okay, sure, word, mass, murder, applies, war,...</td>\n",
       "      <td>[massmurder]</td>\n",
       "      <td>[huntsville, al]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5848</th>\n",
       "      <td>8356</td>\n",
       "      <td>ruin</td>\n",
       "      <td>Garrett</td>\n",
       "      <td>like why on earth would you want anybody to be...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>like why on earth would you want anybody to be...</td>\n",
       "      <td>garrett</td>\n",
       "      <td>ruin</td>\n",
       "      <td>[like, earth, would, want, anybody, unhappy, d...</td>\n",
       "      <td>[ruin]</td>\n",
       "      <td>[garrett]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1851</th>\n",
       "      <td>2661</td>\n",
       "      <td>crush</td>\n",
       "      <td>Cleveland, Ohio</td>\n",
       "      <td>My woman crush wedneday goes to the beautiful ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>108</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>8.083333</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>my woman crush wedneday goes to the beautiful</td>\n",
       "      <td>cleveland ohio</td>\n",
       "      <td>crush</td>\n",
       "      <td>[woman, crush, wedneday, goes, beautiful]</td>\n",
       "      <td>[crush]</td>\n",
       "      <td>[cleveland, ohio]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5729</th>\n",
       "      <td>8176</td>\n",
       "      <td>rescuers</td>\n",
       "      <td>Washington</td>\n",
       "      <td>#News: 'Many deaths' in shipwreck: Rescuers ar...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>136</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>6.611111</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>many deaths in shipwreck rescuers are trying t...</td>\n",
       "      <td>washington</td>\n",
       "      <td>rescuers</td>\n",
       "      <td>[many, deaths, shipwreck, rescuers, trying, sa...</td>\n",
       "      <td>[rescuers]</td>\n",
       "      <td>[washington]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id        keyword                  location  \\\n",
       "6818  9765        trapped  10 Steps Ahead.  Cloud 9   \n",
       "4842  6896  mass%20murder            Huntsville, AL   \n",
       "5848  8356           ruin                   Garrett   \n",
       "1851  2661          crush           Cleveland, Ohio   \n",
       "5729  8176       rescuers                Washington   \n",
       "\n",
       "                                                   text  target  \\\n",
       "6818  Bomb head? Explosive decisions dat produced mo...       1   \n",
       "4842  Okay not sure the word 'mass murder' applies d...       1   \n",
       "5848  like why on earth would you want anybody to be...       0   \n",
       "1851  My woman crush wedneday goes to the beautiful ...       0   \n",
       "5729  #News: 'Many deaths' in shipwreck: Rescuers ar...       1   \n",
       "\n",
       "      presence_location  presence_keyword  char_count  word_count  \\\n",
       "6818                  1                 1         134          21   \n",
       "4842                  1                 1         121          19   \n",
       "5848                  1                 1         101          17   \n",
       "1851                  1                 1         108          12   \n",
       "5729                  1                 1         136          18   \n",
       "\n",
       "      stopword_count  ...  mean_word_length  punctuation_count  hashtag_count  \\\n",
       "6818               6  ...          5.428571                  1              0   \n",
       "4842               9  ...          5.421053                  8              0   \n",
       "5848               6  ...          5.000000                  2              0   \n",
       "1851               3  ...          8.083333                  8              2   \n",
       "5729               6  ...          6.611111                 14              1   \n",
       "\n",
       "      mention_count                                         clean_text  \\\n",
       "6818              0  bomb head explosive decisions dat produced mor...   \n",
       "4842              0  okay not sure the word mass murder applies dur...   \n",
       "5848              0  like why on earth would you want anybody to be...   \n",
       "1851              1      my woman crush wedneday goes to the beautiful   \n",
       "5729              1  many deaths in shipwreck rescuers are trying t...   \n",
       "\n",
       "         clean_location  clean_keyword  \\\n",
       "6818  steps ahead cloud        trapped   \n",
       "4842      huntsville al     massmurder   \n",
       "5848            garrett           ruin   \n",
       "1851     cleveland ohio          crush   \n",
       "5729         washington       rescuers   \n",
       "\n",
       "                                                 tokens keyword_tokens  \\\n",
       "6818  [bomb, head, explosive, decisions, dat, produc...      [trapped]   \n",
       "4842  [okay, sure, word, mass, murder, applies, war,...   [massmurder]   \n",
       "5848  [like, earth, would, want, anybody, unhappy, d...         [ruin]   \n",
       "1851          [woman, crush, wedneday, goes, beautiful]        [crush]   \n",
       "5729  [many, deaths, shipwreck, rescuers, trying, sa...     [rescuers]   \n",
       "\n",
       "            location_tokens  \n",
       "6818  [steps, ahead, cloud]  \n",
       "4842       [huntsville, al]  \n",
       "5848              [garrett]  \n",
       "1851      [cleveland, ohio]  \n",
       "5729           [washington]  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_balanced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97310263",
   "metadata": {},
   "source": [
    "## Application "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36053bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sulivanmoreau/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sulivanmoreau/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/sulivanmoreau/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "\n",
    "# Téléchargements\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initialisation\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Nettoyage de texte\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)      # URLs\n",
    "    text = re.sub(r\"@\\w+|#\\w+\", \"\", text)           # Mentions & hashtags\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)            # Ponctuation / chiffres\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# Appliquer le nettoyage\n",
    "for col in [\"text\", \"keyword\", \"location\"]:\n",
    "    df_balanced[f\"clean_{col}\"] = df_balanced[col].apply(clean_text)\n",
    "    df_balanced[f\"{col}_tokens\"] = df_balanced[f\"clean_{col}\"].apply(\n",
    "        lambda x: [w for w in x.split() if w not in stop_words]\n",
    "    )\n",
    "    df_balanced[f\"{col}_stem\"] = df_balanced[f\"{col}_tokens\"].apply(\n",
    "        lambda tokens: \" \".join([stemmer.stem(t) for t in tokens])\n",
    "    )\n",
    "    df_balanced[f\"{col}_lemma\"] = df_balanced[f\"{col}_tokens\"].apply(\n",
    "        lambda tokens: \" \".join([lemmatizer.lemmatize(t) for t in tokens])\n",
    "    )\n",
    "\n",
    "# Tu peux ensuite créer la version combinée pour la vectorisation\n",
    "df_balanced[\"text_combined\"] = (\n",
    "    df_balanced[\"text_lemma\"] + \" \" +\n",
    "    df_balanced[\"keyword_lemma\"].fillna(\"\") + \" \" +\n",
    "    df_balanced[\"location_lemma\"].fillna(\"\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f0545a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>presence_location</th>\n",
       "      <th>presence_keyword</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>stopword_count</th>\n",
       "      <th>...</th>\n",
       "      <th>keyword_tokens</th>\n",
       "      <th>location_tokens</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>text_stem</th>\n",
       "      <th>text_lemma</th>\n",
       "      <th>keyword_stem</th>\n",
       "      <th>keyword_lemma</th>\n",
       "      <th>location_stem</th>\n",
       "      <th>location_lemma</th>\n",
       "      <th>text_combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6818</th>\n",
       "      <td>9765</td>\n",
       "      <td>trapped</td>\n",
       "      <td>10 Steps Ahead.  Cloud 9</td>\n",
       "      <td>Bomb head? Explosive decisions dat produced mo...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>134</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>[trapped]</td>\n",
       "      <td>[steps, ahead, cloud]</td>\n",
       "      <td>[bomb, head, explosive, decisions, dat, produc...</td>\n",
       "      <td>bomb head explos decis dat produc dead childre...</td>\n",
       "      <td>bomb head explosive decision dat produced dead...</td>\n",
       "      <td>trap</td>\n",
       "      <td>trapped</td>\n",
       "      <td>step ahead cloud</td>\n",
       "      <td>step ahead cloud</td>\n",
       "      <td>bomb head explosive decision dat produced dead...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4842</th>\n",
       "      <td>6896</td>\n",
       "      <td>mass%20murder</td>\n",
       "      <td>Huntsville, AL</td>\n",
       "      <td>Okay not sure the word 'mass murder' applies d...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>121</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>[massmurder]</td>\n",
       "      <td>[huntsville, al]</td>\n",
       "      <td>[okay, sure, word, mass, murder, applies, war,...</td>\n",
       "      <td>okay sure word mass murder appli war horrend n...</td>\n",
       "      <td>okay sure word mass murder applies war horrend...</td>\n",
       "      <td>massmurd</td>\n",
       "      <td>massmurder</td>\n",
       "      <td>huntsvill al</td>\n",
       "      <td>huntsville al</td>\n",
       "      <td>okay sure word mass murder applies war horrend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5848</th>\n",
       "      <td>8356</td>\n",
       "      <td>ruin</td>\n",
       "      <td>Garrett</td>\n",
       "      <td>like why on earth would you want anybody to be...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>[ruin]</td>\n",
       "      <td>[garrett]</td>\n",
       "      <td>[like, earth, would, want, anybody, unhappy, d...</td>\n",
       "      <td>like earth would want anybodi unhappi dont pur...</td>\n",
       "      <td>like earth would want anybody unhappy dont pur...</td>\n",
       "      <td>ruin</td>\n",
       "      <td>ruin</td>\n",
       "      <td>garrett</td>\n",
       "      <td>garrett</td>\n",
       "      <td>like earth would want anybody unhappy dont pur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1851</th>\n",
       "      <td>2661</td>\n",
       "      <td>crush</td>\n",
       "      <td>Cleveland, Ohio</td>\n",
       "      <td>My woman crush wedneday goes to the beautiful ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>108</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>[crush]</td>\n",
       "      <td>[cleveland, ohio]</td>\n",
       "      <td>[woman, crush, wedneday, goes, beautiful]</td>\n",
       "      <td>woman crush wedneday goe beauti</td>\n",
       "      <td>woman crush wedneday go beautiful</td>\n",
       "      <td>crush</td>\n",
       "      <td>crush</td>\n",
       "      <td>cleveland ohio</td>\n",
       "      <td>cleveland ohio</td>\n",
       "      <td>woman crush wedneday go beautiful crush clevel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5729</th>\n",
       "      <td>8176</td>\n",
       "      <td>rescuers</td>\n",
       "      <td>Washington</td>\n",
       "      <td>#News: 'Many deaths' in shipwreck: Rescuers ar...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>136</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>[rescuers]</td>\n",
       "      <td>[washington]</td>\n",
       "      <td>[many, deaths, shipwreck, rescuers, trying, sa...</td>\n",
       "      <td>mani death shipwreck rescuer tri save hundr mi...</td>\n",
       "      <td>many death shipwreck rescuer trying save hundr...</td>\n",
       "      <td>rescuer</td>\n",
       "      <td>rescuer</td>\n",
       "      <td>washington</td>\n",
       "      <td>washington</td>\n",
       "      <td>many death shipwreck rescuer trying save hundr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id        keyword                  location  \\\n",
       "6818  9765        trapped  10 Steps Ahead.  Cloud 9   \n",
       "4842  6896  mass%20murder            Huntsville, AL   \n",
       "5848  8356           ruin                   Garrett   \n",
       "1851  2661          crush           Cleveland, Ohio   \n",
       "5729  8176       rescuers                Washington   \n",
       "\n",
       "                                                   text  target  \\\n",
       "6818  Bomb head? Explosive decisions dat produced mo...       1   \n",
       "4842  Okay not sure the word 'mass murder' applies d...       1   \n",
       "5848  like why on earth would you want anybody to be...       0   \n",
       "1851  My woman crush wedneday goes to the beautiful ...       0   \n",
       "5729  #News: 'Many deaths' in shipwreck: Rescuers ar...       1   \n",
       "\n",
       "      presence_location  presence_keyword  char_count  word_count  \\\n",
       "6818                  1                 1         134          21   \n",
       "4842                  1                 1         121          19   \n",
       "5848                  1                 1         101          17   \n",
       "1851                  1                 1         108          12   \n",
       "5729                  1                 1         136          18   \n",
       "\n",
       "      stopword_count  ...  keyword_tokens        location_tokens  \\\n",
       "6818               6  ...       [trapped]  [steps, ahead, cloud]   \n",
       "4842               9  ...    [massmurder]       [huntsville, al]   \n",
       "5848               6  ...          [ruin]              [garrett]   \n",
       "1851               3  ...         [crush]      [cleveland, ohio]   \n",
       "5729               6  ...      [rescuers]           [washington]   \n",
       "\n",
       "                                            text_tokens  \\\n",
       "6818  [bomb, head, explosive, decisions, dat, produc...   \n",
       "4842  [okay, sure, word, mass, murder, applies, war,...   \n",
       "5848  [like, earth, would, want, anybody, unhappy, d...   \n",
       "1851          [woman, crush, wedneday, goes, beautiful]   \n",
       "5729  [many, deaths, shipwreck, rescuers, trying, sa...   \n",
       "\n",
       "                                              text_stem  \\\n",
       "6818  bomb head explos decis dat produc dead childre...   \n",
       "4842  okay sure word mass murder appli war horrend n...   \n",
       "5848  like earth would want anybodi unhappi dont pur...   \n",
       "1851                    woman crush wedneday goe beauti   \n",
       "5729  mani death shipwreck rescuer tri save hundr mi...   \n",
       "\n",
       "                                             text_lemma  keyword_stem  \\\n",
       "6818  bomb head explosive decision dat produced dead...          trap   \n",
       "4842  okay sure word mass murder applies war horrend...      massmurd   \n",
       "5848  like earth would want anybody unhappy dont pur...          ruin   \n",
       "1851                  woman crush wedneday go beautiful         crush   \n",
       "5729  many death shipwreck rescuer trying save hundr...       rescuer   \n",
       "\n",
       "      keyword_lemma     location_stem    location_lemma  \\\n",
       "6818        trapped  step ahead cloud  step ahead cloud   \n",
       "4842     massmurder      huntsvill al     huntsville al   \n",
       "5848           ruin           garrett           garrett   \n",
       "1851          crush    cleveland ohio    cleveland ohio   \n",
       "5729        rescuer        washington        washington   \n",
       "\n",
       "                                          text_combined  \n",
       "6818  bomb head explosive decision dat produced dead...  \n",
       "4842  okay sure word mass murder applies war horrend...  \n",
       "5848  like earth would want anybody unhappy dont pur...  \n",
       "1851  woman crush wedneday go beautiful crush clevel...  \n",
       "5729  many death shipwreck rescuer trying save hundr...  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_balanced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d7084dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced.to_pickle(\"df_balanced.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eac99f0",
   "metadata": {},
   "source": [
    "## 🔧 Améliorations du nettoyage textuel\n",
    "\n",
    "Votre nettoyage est déjà très bon ! Quelques suggestions d'amélioration :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9ace90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_clean_text(text):\n",
    "    \"\"\"\n",
    "    Nettoyage textuel amélioré pour les tweets\n",
    "    \"\"\"\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # 1. Gestion des contractions anglaises courantes\n",
    "    contractions = {\n",
    "        \"can't\": \"cannot\", \"won't\": \"will not\", \"n't\": \" not\",\n",
    "        \"'re\": \" are\", \"'ve\": \" have\", \"'ll\": \" will\", \"'d\": \" would\",\n",
    "        \"'m\": \" am\", \"it's\": \"it is\", \"that's\": \"that is\"\n",
    "    }\n",
    "    for contraction, expansion in contractions.items():\n",
    "        text = text.replace(contraction, expansion)\n",
    "    \n",
    "    # 2. Nettoyage des URLs et mentions (garder trace du nombre)\n",
    "    url_count = len(re.findall(r\"http\\S+|www\\S+\", text))\n",
    "    mention_count = len(re.findall(r\"@\\w+\", text))\n",
    "    hashtag_count = len(re.findall(r\"#\\w+\", text))\n",
    "    \n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \" URL \", text)      # Remplacer par token\n",
    "    text = re.sub(r\"@\\w+\", \" MENTION \", text)            # Remplacer par token  \n",
    "    text = re.sub(r\"#\\w+\", \" HASHTAG \", text)            # Remplacer par token\n",
    "    \n",
    "    # 3. Caractères répétés (ex: \"nooooo\" → \"no\")\n",
    "    text = re.sub(r\"(.)\\1{2,}\", r\"\\1\", text)\n",
    "    \n",
    "    # 4. Nettoyage final\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    return text, url_count, mention_count, hashtag_count\n",
    "\n",
    "# Exemple d'utilisation\n",
    "sample_text = \"OMG!!! Can't believe this happened 😱 http://news.com @emergency #disaster\"\n",
    "cleaned, urls, mentions, hashtags = enhanced_clean_text(sample_text)\n",
    "print(f\"Original: {sample_text}\")\n",
    "print(f\"Cleaned: {cleaned}\")\n",
    "print(f\"URLs: {urls}, Mentions: {mentions}, Hashtags: {hashtags}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d121c46",
   "metadata": {},
   "source": [
    "## 📊 Validation du nettoyage\n",
    "\n",
    "Toujours vérifier l'impact du preprocessing :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33d9d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation des résultats de nettoyage\n",
    "print(\"🔍 Validation du nettoyage\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Vérifier l'équilibrage des classes\n",
    "print(f\"Distribution des classes après équilibrage :\")\n",
    "print(df_balanced['target'].value_counts(normalize=True))\n",
    "print()\n",
    "\n",
    "# 2. Comparer longueurs avant/après nettoyage\n",
    "print(f\"Longueur moyenne avant nettoyage : {df_balanced['text'].str.len().mean():.1f} caractères\")\n",
    "print(f\"Longueur moyenne après nettoyage : {df_balanced['text_lemma'].str.len().mean():.1f} caractères\")\n",
    "print()\n",
    "\n",
    "# 3. Exemples de transformation\n",
    "print(\"📝 Exemples de nettoyage :\")\n",
    "for i in range(3):\n",
    "    original = df_balanced['text'].iloc[i]\n",
    "    cleaned = df_balanced['text_lemma'].iloc[i]\n",
    "    print(f\"\\nOriginal: {original}\")\n",
    "    print(f\"Nettoyé: {cleaned}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# 4. Statistiques finales\n",
    "print(f\"\\n📊 Dataset final :\")\n",
    "print(f\"- Nombre de lignes : {len(df_balanced)}\")\n",
    "print(f\"- Équilibrage : {df_balanced['target'].value_counts().to_dict()}\")\n",
    "print(f\"- Colonnes créées : text_lemma, keyword_lemma, location_lemma, text_combined\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
