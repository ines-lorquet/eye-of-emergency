# üß† Eye of Emergency ‚Äì Classification de Tweets de Catastrophes

## üìå Contexte

Les r√©seaux sociaux, et notamment Twitter, sont devenus des sources critiques d‚Äôinformations en temps r√©el lors de catastrophes. Cependant, la d√©sinformation y est aussi tr√®s pr√©sente.  
**Objectif :** d√©velopper un mod√®le de classification de tweets capable de distinguer les messages li√©s √† des catastrophes r√©elles de ceux qui ne le sont pas, pour faciliter la diffusion d‚Äôalertes fiables.

---

## üóÉÔ∏è Jeu de donn√©es

Le dataset "Disaster Tweets" contient les colonnes suivantes :

| Colonne   | Description                                          |
|-----------|------------------------------------------------------|
| `id`      | Identifiant unique du tweet                          |
| `text`    | Contenu textuel                                      |
| `location`| Localisation du tweet (souvent bruit√©e)              |
| `keyword` | Mot-cl√© assign√© lors de la collecte                  |
| `target`  | 1 = catastrophe r√©elle, 0 = non-catastrophe          |

Des √©tapes de nettoyage ont √©t√© appliqu√©es : suppression des doublons, des valeurs nulles et des colonnes non informatives (`id`, `location`).

---

## üîç Analyse exploratoire des donn√©es (EDA)

- V√©rification de la distribution des classes (`target`)
- Analyse des longueurs des tweets (caract√®res, mots)
- Analyse lexicale : mots fr√©quents, comparaison des classes
- Visualisations :
  - Nuages de mots
  - Barplots des mots dominants
  - Distribution des longueurs
  - Fr√©quences des keywords
- Observation de diff√©rences lexicales notables entre les classes

---

## üìö Veille sur le NLP

### 1. Text Mining vs NLP
- **Text Mining** : extraction d'informations exploitables √† partir de texte brut (fr√©quence, cooccurrence, etc.)
- **NLP** : ensemble de techniques pour faire comprendre, manipuler ou g√©n√©rer du langage naturel par une machine.
- Le NLP englobe le text mining, mais vise des traitements plus intelligents (compr√©hension, sens).

### 2. Sous-domaines du NLP
- **Analyse de sentiments** : d√©tecter l‚Äôopinion (ex : avis positifs/n√©gatifs)
- **NER (Named Entity Recognition)** : extraire lieux, dates, personnes (ex : ‚ÄúParis‚Äù, ‚ÄúONU‚Äù)
- **POS tagging** : identifier la fonction grammaticale des mots (ex : "ferme" = nom ou verbe)

### 3. Applications concr√®tes
- D√©tection de fake news, alertes automatiques, support client (chatbots), mod√©ration de contenu, analyse de tendances (Twitter), traduction automatique.

### 4. Stop-words
- Mots tr√®s fr√©quents sans int√©r√™t pour la classification (ex : "the", "and")
- Supprim√©s pour √©viter le bruit et am√©liorer la repr√©sentativit√© du texte

### 5. Ponctuation et caract√®res sp√©ciaux
- Nettoy√©s (ex : URLs, mentions, hashtags, √©mojis) pour obtenir un texte exploitable
- On garde parfois les hashtags s‚Äôils sont informatifs (ex : "#earthquake")

### 6. Tokens et N-grams
- **Token** : mot ou unit√© du texte
- **N-gram** : suite de N mots (ex : "feu de for√™t")
- Permettent de capturer le **contexte** (ex : "suicide bomber" plus informatif que "suicide" seul)

### 7. Stemming vs Lemmatization
- **Stemming** : d√©coupe brutale (ex : "flooded" ‚Üí "flood")
- **Lemmatisation** : conversion linguistique pr√©cise (ex : "was" ‚Üí "be")
- Le stemming est plus rapide, la lemmatisation plus pr√©cise

### 8. Repr√©sentation vectorielle
- **Bag-of-Words (BoW)** : repr√©sentation par fr√©quences brutes
- **TF-IDF** : fr√©quence pond√©r√©e par la raret√© (met en valeur les mots sp√©cifiques)
- Ces vecteurs sont utilis√©s comme entr√©e dans les mod√®les de ML

---

## üßπ Pr√©traitement des textes

Pipeline appliqu√© :

1. Conversion en minuscules
2. Suppression :
   - URL, mentions (@), hashtags
   - Ponctuation, chiffres, caract√®res sp√©ciaux
3. Suppression des stop-words
4. Lemmatisation
5. Vectorisation : TF-IDF

---

## ü§ñ Mod√©lisation

### Mod√®les test√©s :
- R√©gression Logistique
- Arbre de D√©cision (y compris impl√©mentation manuelle)
- Random Forest
- XGBoost
- SVM

### Enjeux :
- Comparer bagging (Random Forest) et boosting (XGBoost)
- Optimiser les hyperparam√®tres avec **GridSearchCV**
- Tester diff√©rents pipelines de nettoyage

---

## üìä √âvaluation

- M√©triques : Accuracy, Recall, F1-score
- Objectif prioritaire : **maximiser le F1-score**
- Visualisation : matrice de confusion, classification report

---

## ‚úÖ R√©sultats & Conclusion

- Meilleur mod√®le : [√† compl√©ter avec ton r√©sultat]
- Justification du choix : [ex : bon compromis pr√©cision/recall]
- Limites du mod√®le : tweets ambigus, ironie, usages figuratifs
- Pistes d'am√©lioration :
  - Word embeddings (Word2Vec, GloVe)
  - Mod√®les contextuels (BERT, RoBERTa)

---

## üóÇÔ∏è Contenu du repository

- `README.md` : ce fichier
- `notebook.ipynb` : code structur√© et comment√©
- `presentation.pdf` : slides de soutenance
- Dossier `data/` : fichiers CSV
- Dossier `outputs/` : figures et r√©sultats du mod√®le

---

## üîó R√©f√©rences cl√©s

- [nltk.org/book](https://www.nltk.org/book/)
- [spacy.io](https://spacy.io)
- [Bagging vs Boosting ‚Äì Quantdare](https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/)
- [GridSearch explained](https://www.lovelyanalytics.com/2017/10/16/grid-search/)
- [Text analytics ‚Äì Datacamp](https://www.datacamp.com/tutorial/text-analytics-beginners-nltk)

