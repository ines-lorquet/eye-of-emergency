### Introduction

Le NLP (traitement automatique du langage naturel) est une boÃ®te Ã  outils en intelligence artificielle dÃ©diÃ©e au traitement du langage. Il combine lâ€™apprentissage automatique, les statistiques et les connaissances linguistiques.

Il existe deux grands types dâ€™usage :

    - La comprÃ©hension du langage naturel (NLU)

    - La gÃ©nÃ©ration de langage naturel (NLG)

Le NLP permet de rÃ©aliser des tÃ¢ches comme :

- lâ€™analyse de sentiment,

- la classification de texte,

- les chatbots et assistants virtuels,

- lâ€™extraction dâ€™informations,

- la traduction,

- le rÃ©sumÃ© automatique,

- la reconnaissance vocale

- la correction automatique.

Le Text Mining (exploration de texte) et le Text Analysis (analyse de texte) sont deux approches principales pour construire ces types de projets.

Lâ€™analyse de texte consiste Ã  extraire des informations et des connaissances Ã  partir de texte en utilisant lâ€™IA et les techniques de NLP. Ces techniques transforment les donnÃ©es non structurÃ©es en donnÃ©es structurÃ©es, ce qui facilite le travail des data scientists et analystes.

### Q1) Text mining vs NLP

Le Text Mining et le NLP partagent un objectif commun : tirer du sens Ã  partir de donnÃ©es textuelles. Tous deux sâ€™appuient sur des techniques comme la tokenisation, lâ€™analyse syntaxique, lâ€™extraction dâ€™entitÃ©s ou encore la classification de texte. Ils utilisent les mÃªmes algorithmes (machine learning, deep learning) et manipulent les mÃªmes types de donnÃ©es (texte non structurÃ©). En rÃ©sumÃ©, le Text Mining dÃ©pend fortement du NLP pour fonctionner efficacement, et ils sont souvent utilisÃ©s ensemble dans les projets dâ€™analyse de texte.

Le NLP est lâ€™ensemble des techniques permettant aux machines de comprendre et manipuler le langage humain, tandis que le Text Mining est lâ€™application concrÃ¨te de ces techniques pour extraire de lâ€™information utile Ã  partir de grands volumes de texte. En dâ€™autres termes, le NLP fournit les outils (comme la reconnaissance dâ€™entitÃ©s, la lemmatisation, etc.), et le Text Mining les utilise pour analyser, explorer et structurer les donnÃ©es textuelles dans un but prÃ©cis, comme la prise de dÃ©cision ou la dÃ©tection de tendances.

![Text mining vs Nlp](img/text_mining-nlp.png "Text mining vs Nlp")

- Text Mining : 

<ins>Objectif </ins> : Explorer les textes pour identifier des patterns globaux.

Topic Modeling
â†’ Identifier automatiquement les thÃ¨mes ou sujets rÃ©currents dans les textes (ex : sÃ©curitÃ©, ventilation, accident...).

Co-occurrence Plot
â†’ Visualisation des cooccurrences de mots : quels mots apparaissent frÃ©quemment ensemble ?
Cela donne une idÃ©e des associations sÃ©mantiques dans le corpus.

Le Text Mining aide Ã  explorer, structurer et rÃ©sumer de grandes quantitÃ©s de texte sans forcÃ©ment chercher Ã  "comprendre le sens" aussi finement que le NLP.

- Nlp :

<ins>Objectif</ins> : Comprendre la structure linguistique du texte.

POS Tagging (Part-of-Speech Tagging)
â†’ Ã‰tiqueter chaque mot selon sa fonction grammaticale : nom, verbe, adjectif, etc.

Information Extraction
â†’ Extraire des entitÃ©s prÃ©cises (personnes, lieux, dates, types dâ€™Ã©vÃ©nementsâ€¦) ou relations entre ces entitÃ©s.
Câ€™est une Ã©tape avancÃ©e, qui utilise le POS Tagging et dâ€™autres analyses syntaxiques.

Le NLP permet une comprÃ©hension fine du texte pour en tirer des informations ciblÃ©es.

### Q2) NLP & Sous-domaines

- Analyse de sentiments (Sentiment Analysis)

<ins>Objectif</ins> : DÃ©terminer lâ€™Ã©motion ou lâ€™opinion exprimÃ©e dans un texte (positive, nÃ©gative, neutre).

<ins>Exemple</ins> : â€œLe service Ã©tait vraiment excellent, je reviendrai sans hÃ©siter !â€

â†’ Le modÃ¨le de NLP attribuera un score positif Ã  cette phrase.
Cela peut Ãªtre utilisÃ© pour analyser des milliers dâ€™avis et savoir si un produit ou un service est bien perÃ§u.

<ins>Cas dâ€™usage</ins> :
Analyser les commentaires clients
Suivre la rÃ©putation dâ€™une marque sur les rÃ©seaux sociaux, filtrer des feedbacks nÃ©gatifs automatiquement

- Reconnaissance dâ€™entitÃ©s nommÃ©es (Named Entity Recognition, NER)

<ins>Objectif</ins> : Identifier et extraire des entitÃ©s importantes dans un texte, comme des noms de personnes, lieux, organisations, dates, montants, etc.

<ins>Exemple</ins> : â€œEmmanuel Macron a rencontrÃ© Elon Musk Ã  Paris le 15 juin 2023.â€

â†’ NER dÃ©tecte :
Emmanuel Macron â†’ Personne

Elon Musk â†’ Personne

Paris â†’ Lieu

15 juin 2023 â†’ Date

<ins>Cas dâ€™usage</ins> :

Extraire automatiquement les faits clÃ©s dâ€™un article de presse

Alimenter une base de donnÃ©es avec des noms dÃ©tectÃ©s dans des documents

Faire de la veille informationnelle en dÃ©tectant des noms importants dans des flux dâ€™actualitÃ©


- Ã‰tiquetage grammatical (Part-of-Speech Tagging, POS Tagging)

<ins>Objectif</ins> : Attribuer une catÃ©gorie grammaticale Ã  chaque mot (nom, verbe, adjectif, etc.).

Exemple : â€œLes enfants jouent dans le parc.â€

â†’ RÃ©sultat POS :

Les â†’ dÃ©terminant

enfants â†’ nom

jouent â†’ verbe

dans â†’ prÃ©position

le â†’ dÃ©terminant

parc â†’ nom

<ins>Cas dâ€™usage</ins> :

Faciliter la traduction automatique

AmÃ©liorer la lemmatisation en connaissant le rÃ´le du mot

PrÃ©parer des textes pour lâ€™analyse syntaxique ou sÃ©mantique

### Q3) Uses cases

Le NLP permet de rÃ©aliser des tÃ¢ches comme :

lâ€™analyse de sentiment,

la classification de texte,

les chatbots et assistants virtuels,

lâ€™extraction dâ€™informations,

la traduction,

le rÃ©sumÃ© automatique,

la reconnaissance vocale

la correction automatique.

Un cas avancÃ© pourrait-Ãªtre de faire une classification de document en fonction du vocabulaire pour envoyer Ã  l'Ã©quipe dÃ©diÃ©. Par exemple, un demande de remboursement, -> Ã©quipe Sales. Un problÃ¨me informatique, -> support technique â€¦

### Q4 : Quâ€™est-ce quâ€™un stop-word ? Pourquoi est-il important de supprimer les stop-words ?

En recherche d'information, un stop-word (ou mot vide) est un mot tellement courant qu'il apporte peu ou pas d'information sÃ©mantique utile. Il est donc souvent inutile de lâ€™indexer ou de le prendre en compte dans une recherche.  
En franÃ§ais, des mots vides typiques sont par exemple : Â« le Â», Â« la Â», Â« de Â», Â« du Â», Â« ce Â».

***Exemple :***  

**Version non nettoyÃ©e :**  
Â« Je suis allÃ© Ã  la bibliothÃ¨que avec mon frÃ¨re, mais il n'y avait pas de livres intÃ©ressants. Â»

**Stop-words extraits :**  
je, suis, Ã , la, avec, mon, mais, il, n', y, avait, de

**Version nettoyÃ©e (sans stop-words) :**  
Â« allÃ© bibliothÃ¨que frÃ¨re, livres intÃ©ressants. Â»

Ce traitement est typique dans le prÃ©traitement NLP (traitement automatique du langage naturel) : il permet de se concentrer sur les mots clÃ©s qui portent rÃ©ellement le sens du texte.
  
***Impact avec et sans stop word :***  

![Visualiser l'impact avec et sans](img/stop_word_graph.webp "Visualiser l'impact avec et sans")



### Q5 : En plus de la ponctuation, on retrouve souvent des caractÃ¨res spÃ©ciaux au sein de donnÃ©es textuelles. Comment sont traitÃ©s ces deux types de caractÃ¨res ?

En traitement de texte, la ponctuation et les caractÃ¨res spÃ©ciaux peuvent nuire Ã  lâ€™analyse automatique. Ils compliquent la sÃ©paration correcte des mots (tokenisation) et ajoutent du bruit dans les donnÃ©es, ce qui gÃªne la comprÃ©hension par les machines.

Ces Ã©lÃ©ments n'ont gÃ©nÃ©ralement pas de valeur sÃ©mantique importante, câ€™est pourquoi ils sont souvent supprimÃ©s ou normalisÃ©s lors du prÃ©traitement. Cela permet dâ€™obtenir un texte plus propre, facilitant les tÃ¢ches comme lâ€™analyse de sentiment ou la classification.

**CaractÃ¨res spÃ©ciaux et de ponctuation qui sont souvent supprimÃ©s :**  
| # â€˜â€œ , .% & ^*! @ ( ) _ = â€“ [ ]$ > \ { } ` ~; : / ? <


***Exemple :***  

**Version non nettoyÃ©e :**  
"Salut !!! Tu vas bien ? Moi Ã§a va ğŸ˜Š. On se voit @18h30 â€“ n'oublie pas ğŸ˜‰ #rdv"

**Ã‰tapes de traitement :**  

    - Suppression de la ponctuation excessive (!!!, ?, ., â€“, â€™, etc.)

    - Suppression des emojis (ğŸ˜Š, ğŸ˜‰)

    - Suppression des caractÃ¨res spÃ©ciaux / hashtags / mentions (@, #)

    - Nettoyage des contractions ou symboles non standards 

**Version nettoyÃ©e (sans ponctation et caractÃ¨res spÃ©ciaux):**  
"Salut Tu vas bien Moi Ã§a va On se voit 18h30 n'oublie pas rdv"



### Q6 : Quâ€™est ce quâ€™un token ? un N-gram ? Quel processus permet-il de les obtenir ?

Un token est une unitÃ© de base dâ€™un texte aprÃ¨s dÃ©coupage, souvent un mot, mais cela peut aussi Ãªtre un sous-mot ou un caractÃ¨re selon le contexte.
Un N-gram est une sÃ©quence de n tokens consÃ©cutifs. Par exemple, un bigramme (2-gram) contient deux mots consÃ©cutifs, un trigramme en contient trois, etc.

Ces Ã©lÃ©ments sont obtenus par un processus appelÃ© tokenisation. Câ€™est une Ã©tape du prÃ©traitement en NLP qui dÃ©coupe le texte brut en unitÃ©s exploitables par les algorithmes.

***Exemple Token :***  

**Texte brut :**  
"Les chats dorment souvent au soleil."

**AprÃ¨s tokenisation (tokens) :**  
["Les", "chats", "dorment", "souvent", "au", "soleil"]
![Visualiser l'impact avec et sans](img/Tokenisation.png "Visualiser l'impact avec et sans")

***Exemples N-gram :***  

**Bigrams (2-grams) :**  
["Les chats", "chats dorment", "dorment souvent", "souvent au", "au soleil"]

**Trigrams (3-grams) :**  
["Les chats dorment", "chats dorment souvent", "dorment souvent au", "souvent au soleil"]

![Visualiser l'impact avec et sans](img/n_gram.png "Visualiser l'impact avec et sans")

Ce dÃ©coupage permet aux modÃ¨les de mieux comprendre le contexte et les relations entre les mots dans une phrase.

### Q7) Stemming & Lemmatization
Une fois l'Ã©tape de tokenisation effectuÃ©, il faut commencer l'analyse profond du texte.

- Stemming

Consisite Ã  prendre la <b>racine</b> du mot, en supprimant les sufixes et affixes. Par exemple, les mots "programmation", "programmeur" et "programmes" peuvent Ãªtre Â«transformÃ©sÂ» en <b> programme</b>.


    Limite : Peut trop "gÃ©nÃ©ralisÃ©" un mot, par exemple univers et universitÃ© n'ont pas relations directes.

- Lemmatization

Consiste Ã  analyser le mot dans son <b>contexte grammatical</b> pour le ramener Ã  sa forme de base correcte.
Par exemple, "meilleur" sera associÃ© Ã  "bon", car câ€™est sa forme de base en tant quâ€™adjectif. L'entrainement peut-Ãªtre plus long car l'analyse est plus poussÃ© que celle du Steamming mais apporte des meilleurs rÃ©sultats.


    Avantage : plus prÃ©cis que le stemming, car il comprend la nature grammaticale du mot


En conclusion, la technique du Stemming est plus rapide mais n'a pas de comprÃ©hension du contexte alors que la lemmatization va Ãªtre plus lente mais avec plus de contexte. L'utilisation de l'une ou de l'autre dÃ©pend du projet, une balance entre performance et optimization doit Ãªtre faite.

### Q8) Bag of word & TF - IDF

Bag of word : Les mots sont des kernels. On peut utiliser cet algorithme dans d'autres domaines comme la gÃ©nÃ©ration d'image.
Les uses cases peuvent Ãªtre de la classification, de la similaritÃ© de documents. 

Il prend un corpus de texte, crÃ©er un object contenant tous les mots "uniques" dans un document term matrix. Ensuite on attribue des score sur les occurences du termes. On crÃ©e ensuite des vecteurs qui compte l'occurence des mots par documents.

Exemple : 
1] I think therefore I am
2] I love learning python
Transformation : 
   I think therefore am love learning python
1] 2   1     1        1   0     0       0
2] 1   0     0        0   1     1       1


Bon : C'est simple, facile et expliquable
Mauvais : perte du sens sÃ©mantique, par exemple : New York != NY 
Il ne prend pas en compte la corrÃ©lation des mots.
Il ne peut pas comprendre le sens, ex, Python, animal ou machine learning programming language ?
C'est compliquÃ© de comprendre l'ordre des mots avec la term matrix.
ProblÃ¨me de matrice vide, si beaucoup de documents, les matrices vont contenir Ã©normement de 0

AmÃ©liorations possibles :

On peut utiliser des n-grams pour plus de contexte.
Text normalization, en utilisant le stamming par exemple, pour rÃ©duire le nombre de vocabulaire.

TF-IDF :

Cet algorithme est la version amÃ©liorÃ© de BoW, ici, il y a un systÃ¨me de scoring - poid associÃ© Ã  chaque mots.

Tf dÃ©signe term frequency, en comptant l'occurence de fois que le mot Ã  Ã©tÃ© utilisÃ© (key : occurence). Cela permet d'avoir une idÃ©e, en fonction des nombres frÃ©quents, du thÃ¨me ou du sujet.

IDF dÃ©signe quand Ã  lui le nombre de fois que le mot figure dans les diffÃ©rents documents. Plus le mot figure dans diffÃ©rents documents, plus le score de ce dernier diminue.

Cela permet de rÃ©duire le poid des dÃ©terminants (le, la, un â€¦).
---
Sources : 
https://relativeinsight.com/text-mining-vs-nlp/
https://www.youtube.com/watch?v=fLvJ8VdHLA0
https://www.youtube.com/watch?v=pF9wCgUbRtc